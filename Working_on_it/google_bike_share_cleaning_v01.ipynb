{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04264a8-0315-451d-bb89-3d249b0279af",
   "metadata": {},
   "source": [
    "##### Last update: april 03 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f89cd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bike Share Project - Data Cleaning\n",
    "\n",
    "In this document the data is colected, cleaned and transformed for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab8af5-f89e-4c24-9ad9-3143913bec73",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Data Overview](#overview)\n",
    "3. [Data Cleaning](#cleaning)\n",
    "    * [Column re-naming](#renaming)\n",
    "    * [Missing Data](#missing)\n",
    "    * [Data Validation](#validation)\n",
    "4. [Feature Engineering](#feature)\n",
    "    * [Date](#date)\n",
    "    * [Duration](#duration)\n",
    "    * [Day of Week](#dow)\n",
    "    * [Distance Travelled](#distance)\n",
    "    * [Speed](#speed)\n",
    "5. [Outliers](#outliers)\n",
    "    * [Duration, speed and distance](#dynamics)\n",
    "    * [Start Station Coordinates](#start_coords)\n",
    "    * [End Station Coordinates](#end_coords)\n",
    "6. [Verification](#verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76e524-953c-41bb-8c41-57488d662aca",
   "metadata": {},
   "source": [
    "# Imports <a name=\"imports\"></a>\n",
    "\n",
    "The usual libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ecd79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c504ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Overview <a name=\"overview\"></a>\n",
    "\n",
    "The data is stored in 12 datasets, each one concerning each of the previous 12 months from the date when this study was performed (March 2023). These contain in combination information for almost 6 million bike rides made between March 01 2022 and February 28 2023, which can be found [here](https://divvy-tripdata.s3.amazonaws.com/index.html). \n",
    "\n",
    "Every dataset has the same structure, with each row being a bike ride and having the following fields:\n",
    "\n",
    "* **trip_id**: a unique identifier for each ride made with one of the bikes.\n",
    "* **rideable_type**: the type of bike that was used. The types of bikes are electric, classic and docked.\n",
    "* **started_at, ended_at**: the time and date the ride started/ended. \n",
    "* **start_station_name, end_station_name**: the name of the station where the trip started/ended.\n",
    "* **start_station_id, end_station_id**: the code that identifies the station where the trip started/ended. \n",
    "* **start_lat, start_lng, end_lat, end_lng**: the coordinates for the station where the trip started/ended.\n",
    "* **member_casual**: the type of user, casual or member.\n",
    "\n",
    "To simplify the cleaning process, all 12 datasets are merged into a single, large dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b85544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# collect all file names\n",
    "csv_files = glob.glob(\"../data/*.csv\")\n",
    "\n",
    "# read each of them into a pd.DataFrame and put them into a list\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# concatenate all dataframes, to get a single, large\n",
    "# dataframe with all year's data\n",
    "all_data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3dea9",
   "metadata": {},
   "source": [
    "And a copy of it is made to avoid having to load and merge all files again if something happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3056cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f80ad6",
   "metadata": {},
   "source": [
    "`rides` is our complete datafrme. The following shows the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6efa5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3808929</th>\n",
       "      <td>A97019586F0403A2</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-09-22 13:35:10</td>\n",
       "      <td>2022-09-22 13:41:20</td>\n",
       "      <td>Halsted St &amp; Polk St</td>\n",
       "      <td>TA1307000121</td>\n",
       "      <td>Throop St &amp; Taylor St</td>\n",
       "      <td>13139</td>\n",
       "      <td>41.871795</td>\n",
       "      <td>-87.646588</td>\n",
       "      <td>41.868968</td>\n",
       "      <td>-87.659141</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080915</th>\n",
       "      <td>15F89969916F0372</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-09-29 18:26:13</td>\n",
       "      <td>2022-09-29 18:28:31</td>\n",
       "      <td>Sheridan Rd &amp; Buena Ave</td>\n",
       "      <td>TA1309000027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.958493</td>\n",
       "      <td>-87.655020</td>\n",
       "      <td>41.960000</td>\n",
       "      <td>-87.650000</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549448</th>\n",
       "      <td>F890B691E42CCFD6</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-06-25 00:27:13</td>\n",
       "      <td>2022-06-25 00:58:43</td>\n",
       "      <td>Halsted St &amp; Roscoe St</td>\n",
       "      <td>TA1309000025</td>\n",
       "      <td>Racine Ave &amp; Randolph St</td>\n",
       "      <td>13155</td>\n",
       "      <td>41.943670</td>\n",
       "      <td>-87.648950</td>\n",
       "      <td>41.884069</td>\n",
       "      <td>-87.656853</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ride_id  rideable_type           started_at  \\\n",
       "3808929  A97019586F0403A2  electric_bike  2022-09-22 13:35:10   \n",
       "4080915  15F89969916F0372  electric_bike  2022-09-29 18:26:13   \n",
       "1549448  F890B691E42CCFD6   classic_bike  2022-06-25 00:27:13   \n",
       "\n",
       "                    ended_at       start_station_name start_station_id  \\\n",
       "3808929  2022-09-22 13:41:20     Halsted St & Polk St     TA1307000121   \n",
       "4080915  2022-09-29 18:28:31  Sheridan Rd & Buena Ave     TA1309000027   \n",
       "1549448  2022-06-25 00:58:43   Halsted St & Roscoe St     TA1309000025   \n",
       "\n",
       "                 end_station_name end_station_id  start_lat  start_lng  \\\n",
       "3808929     Throop St & Taylor St          13139  41.871795 -87.646588   \n",
       "4080915                       NaN            NaN  41.958493 -87.655020   \n",
       "1549448  Racine Ave & Randolph St          13155  41.943670 -87.648950   \n",
       "\n",
       "           end_lat    end_lng member_casual  \n",
       "3808929  41.868968 -87.659141        member  \n",
       "4080915  41.960000 -87.650000        member  \n",
       "1549448  41.884069 -87.656853        casual  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64be8d3",
   "metadata": {},
   "source": [
    "As mentioned above, there are almost 6 million rows of data, with 13 columns containing information about the type of bike and user as well as the time and location where each trip started and ended. As can be seen below, there are some errors regarding the data types. Specifically, the dates and times are stored as string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0f62a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5829084 entries, 0 to 5829083\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 578.1+ MB\n"
     ]
    }
   ],
   "source": [
    "rides.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a127f",
   "metadata": {},
   "source": [
    "The only numeric fields are those for the coordinates, for which the statistical summaries are not too useful. It is worth noting, though, that there are some inconsistencies within the end station coordinates, showing latitudes and longitudes going all the way to 0Â°. If this was correct then there would have been bikes stationed within an area of almost 1/16th of the surface of the Earth. Who would have thought Chicago was so big?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c6f545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.829084e+06</td>\n",
       "      <td>5.829084e+06</td>\n",
       "      <td>5.823146e+06</td>\n",
       "      <td>5.823146e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.190217e+01</td>\n",
       "      <td>-8.764779e+01</td>\n",
       "      <td>4.190237e+01</td>\n",
       "      <td>-8.764786e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.626503e-02</td>\n",
       "      <td>2.927663e-02</td>\n",
       "      <td>6.757085e-02</td>\n",
       "      <td>1.068591e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.164000e+01</td>\n",
       "      <td>-8.784000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-8.814000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.188103e+01</td>\n",
       "      <td>-8.766150e+01</td>\n",
       "      <td>4.188103e+01</td>\n",
       "      <td>-8.766201e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.190000e+01</td>\n",
       "      <td>-8.764411e+01</td>\n",
       "      <td>4.190000e+01</td>\n",
       "      <td>-8.764414e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.193000e+01</td>\n",
       "      <td>-8.762963e+01</td>\n",
       "      <td>4.193000e+01</td>\n",
       "      <td>-8.762963e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.207000e+01</td>\n",
       "      <td>-8.752000e+01</td>\n",
       "      <td>4.237000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          start_lat     start_lng       end_lat       end_lng\n",
       "count  5.829084e+06  5.829084e+06  5.823146e+06  5.823146e+06\n",
       "mean   4.190217e+01 -8.764779e+01  4.190237e+01 -8.764786e+01\n",
       "std    4.626503e-02  2.927663e-02  6.757085e-02  1.068591e-01\n",
       "min    4.164000e+01 -8.784000e+01  0.000000e+00 -8.814000e+01\n",
       "25%    4.188103e+01 -8.766150e+01  4.188103e+01 -8.766201e+01\n",
       "50%    4.190000e+01 -8.764411e+01  4.190000e+01 -8.764414e+01\n",
       "75%    4.193000e+01 -8.762963e+01  4.193000e+01 -8.762963e+01\n",
       "max    4.207000e+01 -8.752000e+01  4.237000e+01  0.000000e+00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7718b",
   "metadata": {},
   "source": [
    "Finaly, the result below shows the number of duplicated rows. We can see there aren't any: each row has an unique ride id, which means it is a unique bike trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc582001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.duplicated(subset=[\"ride_id\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8189459",
   "metadata": {},
   "source": [
    "# Data Cleaning <a name=\"cleaning\"></a>\n",
    "\n",
    "## Column re-naming <a name=\"renaming\"></a>\n",
    "\n",
    "The column names can be changed into shorter and more intuitive ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada094e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.rename(\n",
    "    columns={\n",
    "        \"ride_id\": \"trip_id\",\n",
    "        \"rideable_type\": \"bike_type\",\n",
    "        \"started_at\": \"start_time\",\n",
    "        \"ended_at\": \"end_time\",\n",
    "        \"start_station_name\": \"start_name\",\n",
    "        \"start_station_id\": \"start_id\",\n",
    "        \"end_station_name\": \"end_name\",\n",
    "        \"end_station_id\": \"end_id\",\n",
    "        \"member_casual\": \"usertype\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130596ec",
   "metadata": {},
   "source": [
    "## Missing Data <a name=\"missing\"></a>\n",
    "\n",
    "There are values missing from some fields. The following table represents what percentage of data is missing from each field. \n",
    "There is a considerable proportion of start/end station names and id's missing and only a small amount of latitud/longitude data missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rides.isnull().sum() / len(rides) * 100).reset_index().rename(\n",
    "    columns={\"index\": \"Field\", 0: \"% of missing data\"}\n",
    ").set_index(\"Field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca345",
   "metadata": {},
   "source": [
    "The missing station names and id's can be explained with the fact that e-bikes don't need to be docked at stations, they can be locked at public bike racks, light poles, signposts, etc. \n",
    "The following confirms this is the case for most of the rides with missing names and id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\"start_id\", \"start_name\", \"end_id\", \"end_name\"]:\n",
    "    print(\n",
    "        f\"count of missing {var}:\\n\",\n",
    "        rides[rides[var].isnull()]\n",
    "        .groupby(\"bike_type\")[\"trip_id\"]\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"trip_id\": \"\"}),\n",
    "        \"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5119d64",
   "metadata": {},
   "source": [
    "There are a few trips that were made with classic bikes that started or ended at no specified station. We can safely discard those since they account for a really small percentage of the total trips (around 0.002%). As for the trips made with e-bikes that were parked at no specific station, we can state they were parked at \"no_station\", with id 99999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in [\"start_name\", \"end_name\"]:\n",
    "    rides.loc[\n",
    "        (rides[\"bike_type\"] == \"electric_bike\") & (rides[val].isna()), val\n",
    "    ] = \"No station\"\n",
    "\n",
    "for val in [\"start_id\", \"end_id\"]:\n",
    "    rides.loc[\n",
    "        (rides[\"bike_type\"] == \"electric_bike\") & (rides[val].isna()), val\n",
    "    ] = \"99999\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf2e2c",
   "metadata": {},
   "source": [
    "This almost reduces the missing data to 0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64924bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"% of missing data: \", rides.isnull().sum().sum() / len(rides) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815925a",
   "metadata": {},
   "source": [
    "The remainding missing values ammount to less than 0.5% of all the data, so they can be safely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dcd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete = rides.dropna().copy()\n",
    "print(\"% of missing data: \", rides_complete.isnull().sum().sum() / len(rides) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b572b",
   "metadata": {},
   "source": [
    "## Data Validation <a name=\"validation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99883872",
   "metadata": {},
   "source": [
    "There are suposed to be around 600 stations in total, 3 types of bikes (docked, classic and e-bikes) and 2 types of users (casual and members). The following table shows that, according to the data, that is not the case for the station names and ids. Even the ratio of station names to ids is inconsistent. \n",
    "\n",
    "I will postpone a test for the consistency of coordinates, since as we have seen they also refer to bikes that were not parked at any station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete[\n",
    "    [\"bike_type\", \"usertype\", \"start_name\", \"start_id\", \"end_name\", \"end_id\"]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f0e6c",
   "metadata": {},
   "source": [
    "There are a lot more stations names and id's than shoud be. A check for the consistency within stations is then needed; The following function will test if a name or id relates to multiple id's or names. It is performed only on a sample of the data since it is very timeconsuming to run it on larger scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb525d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_unique(field1, field2, frac):\n",
    "    \"\"\"This function takes in 2 column names, field1 and field2, as well\n",
    "    as the fraction, frac, of the dataframe that will be processed, and \n",
    "    returns a dictionary with the field1 values that refer to multiple\n",
    "    unique values for field2 and the corresponding list of field2\n",
    "    values as key-value pairs.\"\"\"\n",
    "    subset = rides_complete.sample(frac=frac)\n",
    "    id_names = {}\n",
    "    for val in ridesf[field1].unique():\n",
    "        if ridesf[ridesf[field1] == val][field2].nunique() > 1:\n",
    "            id_names[val] = ridesf[ridesf[field1] == val][field2].unique()\n",
    "    return id_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815a8f5-e855-43e1-927a-024839e19833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiple_field2_values(df, field1, field2, frac):\n",
    "    # Get a random subset of the dataframe to process based on the given fraction\n",
    "    subset = df.sample(frac=frac)\n",
    "\n",
    "    # Group the subset by field1 and count the number of unique field2 values in each group\n",
    "    counts = subset.groupby(field1)[field2].nunique()\n",
    "\n",
    "    # Filter the groups to only include those with multiple unique field2 values\n",
    "    multiple_values = counts[counts > 1].index.tolist()\n",
    "\n",
    "    # For each group with multiple unique field2 values, get a list of the corresponding field2 values\n",
    "    result = {}\n",
    "    for value in multiple_values:\n",
    "        subset = df[df[field1] == value]\n",
    "        field2_values = subset[field2].unique().tolist()\n",
    "        result[value] = field2_values\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32139538",
   "metadata": {},
   "source": [
    "The next two blocks of code show the number of id's that refer to two or more station names within a sample of 5% of the data, as well as the names of the stations for each of these id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b06cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeated_ids = find_multiple_field2_values(\n",
    "    rides_complete, \"start_id\", \"start_name\", 0.05\n",
    ")\n",
    "len(repeated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d24180",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "repeated_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016a70e",
   "metadata": {},
   "source": [
    "Many id's do refer to different station names, but many of them have a version with \"Public Rack - \" or with \" (Temp)\" in the name and a version without. \n",
    "\n",
    "The next lines fix this so that the data is a bit more homogenous and the analisys more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete[\"start_name\"] = (\n",
    "    rides_complete[\"start_name\"]\n",
    "    .str.replace(\"Public Rack - \", \"\")\n",
    "    .str.replace(\" (Temp)\", \"\")\n",
    ")\n",
    "rides_complete[\"end_name\"] = (\n",
    "    rides_complete[\"end_name\"]\n",
    "    .str.replace(\"Public Rack - \", \"\")\n",
    "    .str.replace(\" (Temp)\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eeb41f",
   "metadata": {},
   "source": [
    "And Lets see how many station names we are left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca124f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_complete[\n",
    "    [\"bike_type\", \"usertype\", \"start_name\", \"start_id\", \"end_name\", \"end_id\"]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88df33",
   "metadata": {},
   "source": [
    "Its a little bit better, not a lot. The following does the same but the other way arround: shows station names that point to multiple id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_names = not_unique(\"start_name\", \"start_id\", 0.05)\n",
    "len(repeated_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f917545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeated_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del repeated_ids, repeated_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cafbd",
   "metadata": {},
   "source": [
    "rides_completeo pattern here. Maybe the stations changed names/id's at some point? Let's investigate how many unique station names/id's are there per month to check this hypothesis. \n",
    "\n",
    "For this purpose, it will be easiest to convert the dates into datetime objects and then get the year and month, but first it is necessary to verify the date format is consistent throughout the dataframe. The next lists show the dates that do not start with either the year 2022 or 2023. Since they are both empty it is safe to say that the format is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[date for date in rides_complete[\"start_time\"] if date[:4] not in [\"2022\", \"2023\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da3176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[date for date in rides_complete[\"end_time\"] if date[:4] not in [\"2022\", \"2023\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c94b5e",
   "metadata": {},
   "source": [
    "Next the dates are typecasted as datetime objects and the the year, month and time of day extracted since it might be interesting to study the hourly usage of the bikes in the analysis phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91649e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete[\"start_time\"] = pd.to_datetime(\n",
    "    rides_complete[\"start_time\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "rides_complete[\"end_time\"] = pd.to_datetime(\n",
    "    rides_complete[\"end_time\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete = rides_complete.assign(year=rides_complete[\"start_time\"].dt.year)\n",
    "rides_complete = rides_complete.assign(month=rides_complete[\"start_time\"].dt.month)\n",
    "rides_complete = rides_complete.assign(time_of_day=rides_complete[\"start_time\"].dt.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8786c",
   "metadata": {},
   "source": [
    "And since it would be more comfortable to see the months with their names, we can make a mapping and enforce a chronological order into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611596a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {\n",
    "    1: \"Jan\",\n",
    "    2: \"Feb\",\n",
    "    3: \"Mar\",\n",
    "    4: \"Apr\",\n",
    "    5: \"May\",\n",
    "    6: \"Jun\",\n",
    "    7: \"Jul\",\n",
    "    8: \"Aug\",\n",
    "    9: \"Sep\",\n",
    "    10: \"Oct\",\n",
    "    11: \"Nov\",\n",
    "    12: \"Dec\",\n",
    "}\n",
    "month_order = [\n",
    "    \"Mar\",\n",
    "    \"Apr\",\n",
    "    \"May\",\n",
    "    \"Jun\",\n",
    "    \"Jul\",\n",
    "    \"Aug\",\n",
    "    \"Sep\",\n",
    "    \"Oct\",\n",
    "    \"Nov\",\n",
    "    \"Dec\",\n",
    "    \"Jan\",\n",
    "    \"Feb\",\n",
    "]\n",
    "rides_complete[\"month\"] = (\n",
    "    rides_complete[\"month\"]\n",
    "    .map(months)\n",
    "    .astype(pd.api.types.CategoricalDtype(categories=month_order, ordered=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65be37",
   "metadata": {},
   "source": [
    "With these fields it is posible to count how many station names and id's are found per month for the last 12 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete.groupby(\"month\")[\n",
    "    [\"start_name\", \"start_id\", \"end_name\", \"end_id\"]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36238c",
   "metadata": {},
   "source": [
    "The number of stations is not accurate nor consistent on any of the fields, since the number of stations is in reallity around 600, which is half to 3/4 of the monthly reported stations on the data. To homogenize things a little bit, we proceed to make the mapping of station names to id's one to one (or as much as possible). Since there is a lower number of id's than that of names, we use first the id's to reduce the station names. Then we will use the names to further narrow down the id's. This doesn't guarantee that the relation will be exactly one to one, but it will clean up most of the mess with a single id or name value pointing to multiple names or id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3e7a9-e17d-4d7a-92b4-cf993fd28c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rides_complete.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ids in test[\"start_id\"].unique():\n",
    "#     if ids != \"99999\":\n",
    "#         if len(test[test[\"start_id\"] == ids][\"start_name\"].unique()) > 1:\n",
    "#             test.loc[test[\"start_id\"] ==\n",
    "#                                ids, \"start_name\"] = np.sort(test.loc[test[\"start_id\"]\n",
    "#                                                                                == ids, \"start_name\"].unique())[0]\n",
    "mask = test[\"start_id\"] != \"99999\"\n",
    "has_multiple_names = test.groupby(\"start_id\")[\"start_name\"].transform(\"nunique\") > 1\n",
    "mask &= has_multiple_names\n",
    "unique_names = test[mask].groupby(\"start_id\")[\"start_name\"].apply(lambda x: x.unique())\n",
    "first_name = unique_names.apply(lambda x: np.sort(x)[0])\n",
    "test.loc[mask, \"start_name\"] = first_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af048d0-5f8e-41f6-ba0e-d093a5a3e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby(\"month\")[[\"start_name\", \"start_id\", \"end_name\", \"end_id\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb8334-bb35-4919-a80b-0bd745686511",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby(\"month\")[[\"start_name\", \"start_id\", \"end_name\", \"end_id\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44977058-e76d-4a1e-92bb-7b692007b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in test[\"start_name\"].unique():\n",
    "#     if name != \"No station\":\n",
    "#         if len(test[test[\"start_name\"] == name][\"start_id\"].unique()) > 1:\n",
    "#             test.loc[test[\"start_name\"] ==\n",
    "#                                name, \"start_id\"] = np.sort(test.loc[test[\"start_name\"]\n",
    "#                                                                               == name, \"start_id\"].unique())[0]\n",
    "mask = test[\"start_name\"] != \"No station\"\n",
    "has_multiple_ids = test.groupby(\"start_name\")[\"start_id\"].transform(\"nunique\") > 1\n",
    "mask &= has_multiple_ids\n",
    "unique_ids = test[mask].groupby(\"start_name\")[\"start_id\"].apply(lambda x: x.unique())\n",
    "first_id = unique_ids.apply(lambda x: np.sort(x)[0])\n",
    "test.loc[mask, \"start_id\"] = first_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311a50d-0155-47a4-8309-9cfc5b1155fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby([\"month\"])[\"start_name\", \"start_id\", \"end_name\", \"end_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388e3b2",
   "metadata": {},
   "source": [
    "And now for the end stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [*test[\"start_name\"].unique()]\n",
    "id_list = [*test[\"start_id\"].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f24d33-ea07-4d51-b134-3fc00a8f93fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name_to_id = dict(test.groupby(\"start_name\")[\"start_id\"].unique())\n",
    "name_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, \"end_id\"] = test[\"end_name\"].map(name_id)\n",
    "\n",
    "# for ids in test[\"end_id\"].unique():\n",
    "#     if ids in id_list:\n",
    "#         test.loc[test[\"end_id\"] ==\n",
    "#                            ids, \"end_name\"] = test[test[\"start_id\"]\n",
    "#                                                              ==  ids][\"start_name\"].unique()[0]\n",
    "# create a mask for the end ids to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ea48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in rides_complete[\"end_name\"].unique():\n",
    "    if name in names:\n",
    "        rides_complete.loc[\n",
    "            rides_complete[\"end_name\"] == name, \"end_id\"\n",
    "        ] = rides_complete[rides_complete[\"start_name\"] == name][\"start_id\"].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16801532",
   "metadata": {},
   "outputs": [],
   "source": [
    "del names, id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72907c",
   "metadata": {},
   "source": [
    "After this we at least have some consistency: the number of names and id's for start/end stations is the same within each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc619ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete.groupby([\"month\"])[\n",
    "    \"start_name\", \"start_id\", \"end_name\", \"end_id\"\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dadad",
   "metadata": {},
   "source": [
    "And the total number of names and ids for start and end stations are almost exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_complete[[\"start_name\", \"end_name\", \"start_id\", \"end_id\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5855e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized = rides_complete.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46434bc6",
   "metadata": {},
   "source": [
    "There still are a lot more stations than there should be but at least now the relation of station names to id's is (almost) 1 to 1 and they are standardized for start and end stations.\n",
    "\n",
    "\n",
    "Next, there are multiple sets of coordinates for each station, differing only by a small amount between them, as can be seen below for one single station having over 5000 values for its latitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb744073",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized[[\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65146189",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized[rides_homogenized[\"start_name\"] == \"State St & Kinzie St\"][\n",
    "    \"start_lat\"\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd06cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized[rides_homogenized[\"start_id\"] == \"13042\"][\"start_lat\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5512f9",
   "metadata": {},
   "source": [
    "Before we implement any solution it might be a good idea to address the issue with the end station coordinate ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized[[\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"]].agg(\n",
    "    [\"max\", \"min\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443660f",
   "metadata": {},
   "source": [
    "The ranges shoud be roughly the same but for some reason it is very different for the end stations. The coordinate range shown above accounts for roughly 1/16th of the Earth's surface area, so there is something fishy going on. \n",
    "\n",
    "After checking the data with trips ending in coordinates outside the range for the start stations we find that there is a minimal amount of rides that fall outside that range, and their coordinates all have a value of 0.0, 0.0. Also, all of them ended at Green St & Madison Ave* station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d02c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized[\n",
    "    (rides_homogenized[\"end_lat\"] < 40) | (rides_homogenized[\"end_lng\"] > -87)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0940b8",
   "metadata": {},
   "source": [
    "It is easy to confirm that this is not the case for all records of that station; there are other instances where the coordinates do fall within the range, so we can use any of those as a replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb6016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_homogenized[rides_homogenized[\"end_name\"] == \"Green St & Madison Ave*\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695287c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized.loc[rides_homogenized[\"end_lat\"] < 40, \"end_lat\"] = 41.881827\n",
    "rides_homogenized.loc[rides_homogenized[\"end_lng\"] > -86.5, \"end_lng\"] = -87.648832"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19658a23",
   "metadata": {},
   "source": [
    "With this we are left with a more sensible range of coordinates for all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea88d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_homogenized[[\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"]].agg(\n",
    "    [\"min\", \"max\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2f83b",
   "metadata": {},
   "source": [
    "Now that this has been taken care of, one posible solution to the issue of the multiple sets of coordinates per station is to use the average of each stations coordinates to figure out the average coordinate \"distance\" between them. This will allow us to determine at which decimal point could we round up the coordinates without mixing up different stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8c30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_coords = (\n",
    "    rides_homogenized[rides_homogenized[\"start_name\"] != \"No station\"]\n",
    "    .groupby(\"start_name\")[[\"start_lat\", \"start_lng\"]]\n",
    "    .mean()\n",
    "    .rename(columns={\"start_lat\": \"lat\", \"start_lng\": \"lng\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a683343",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_coords.sort_values(\"lat\")[\"lat\"].diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107700f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_coords.sort_values(\"lng\")[\"lng\"].diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rides_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d4b45",
   "metadata": {},
   "source": [
    "From these results it looks like the 5th decimal point would give a good amount of precision to differentiate stations, while elimination many of the small vatiations within each station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coord in [\"start_lat\", \"start_lng\"]:\n",
    "    rides_homogenized[coord] = round(rides_homogenized[coord], 5)\n",
    "\n",
    "for coord in [\"end_lat\", \"end_lng\"]:\n",
    "    rides_homogenized[coord] = round(rides_homogenized[coord], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aca29e",
   "metadata": {},
   "source": [
    "And here it is again the number of unique values for each field after these modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be98fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_homogenized[\n",
    "    [\n",
    "        \"start_name\",\n",
    "        \"end_name\",\n",
    "        \"start_id\",\n",
    "        \"end_id\",\n",
    "        \"start_lat\",\n",
    "        \"end_lat\",\n",
    "        \"start_lng\",\n",
    "        \"end_lng\",\n",
    "    ]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d5e31",
   "metadata": {},
   "source": [
    "It is by no means perfect, but it will be good enough for analysis. Some further study on the actual stations, their current names and ids as well as their locations, could help make the data more congruent with real life. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean = rides_homogenized.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977574e",
   "metadata": {},
   "source": [
    "# Feature Engineering <a name=\"feature\"></a>\n",
    "\n",
    "There are a couple fields that might be useful to have, such as the pure date for each trip, the day of week when it happened and the distance travelled. The next subsections are dedicated to these fields. \n",
    "\n",
    "## Date <a name=\"date\"></a>\n",
    "\n",
    "We already have a datetime object from which we can extract the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"date\"] = rides_clean[\"start_time\"].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d27742",
   "metadata": {},
   "source": [
    "## Duration <a name=\"duration\"></a>\n",
    "\n",
    "In short, how much time passed from the moment the ride started to the moment it ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"duration\"] = rides_clean[\"end_time\"] - rides_clean[\"start_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec45971",
   "metadata": {},
   "source": [
    "For consistency we check for any negative durations, since they don't really make sense in this scenario, as well as really long ones. The following lines show that there was a 23 day ride and, more importantly, there is in fact at least one trip with a negative duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b91e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"duration\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76230792",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"duration\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c62807",
   "metadata": {},
   "source": [
    "Since there is at least one acount of a trip with a negative duration, it is worth studying if this is an extended issue or if it is just a few exceptions. The percentage of trips that have a negative duration is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6974e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rides_clean[rides_clean[\"duration\"] < timedelta(seconds=0)]) / len(\n",
    "    rides_clean\n",
    ") * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520c91f",
   "metadata": {},
   "source": [
    "Since the percentage is minimal, we can safely discard these records. Also, the bike sharing website sugests that the data should be purged of rows with a duration of less than 60 seconds, since those could be fake starts or servicing tasks made by the staff. The next line of code removes both cases at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ef7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean = rides_clean[rides_clean[\"duration\"] >= timedelta(seconds=60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a2772",
   "metadata": {},
   "source": [
    "And for visualization purposes the duration as an int field might make things easier, so the next line calculates the duration of the ride in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3131894",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"minutes\"] = rides_clean[\"duration\"].apply(\n",
    "    lambda x: round(x.total_seconds() / 60, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e60263",
   "metadata": {},
   "source": [
    "## Day of Week <a name=\"dow\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d66b8",
   "metadata": {},
   "source": [
    "Different types of users might use the bikes on different days of the week. To test this we will need to know the day of the week for each trip. The date for the beginning of the trip is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a955045",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = {\n",
    "    0: \"Monday\",\n",
    "    1: \"Tuesday\",\n",
    "    2: \"Wednesday\",\n",
    "    3: \"Thursday\",\n",
    "    4: \"Friday\",\n",
    "    5: \"Saturday\",\n",
    "    6: \"Sunday\",\n",
    "}\n",
    "days_order = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae437ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"day_of_week\"] = (\n",
    "    rides_clean[\"start_time\"]\n",
    "    .map(lambda x: x.weekday())\n",
    "    .map(days)\n",
    "    .astype(pd.api.types.CategoricalDtype(categories=days_order, ordered=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3bc5a",
   "metadata": {},
   "source": [
    "## Distance Travelled <a name=\"distance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb664a4",
   "metadata": {},
   "source": [
    "The distance from start to end station can be found using pythagoras theorem,  but to convert from latitudes and longitudes to meters or km it requires an additional step. \n",
    "\n",
    "### Latitude\n",
    "The north-south location is determined by its latitude, which is meassured from the equator, from 0Â° to $\\pm$90Â°. \n",
    "The vertical distance, $d_{lat}$, approximating the Earth as a perfect sphere, is given by Earth's radius, $R\\approx 6371 km$, and by the difference in latitude, $\\Delta\\phi$, measured in radians, that is\n",
    "\n",
    "<p style=\"text-align: center;\"> $d_{lat} = R\\Delta\\phi \\frac{\\pi}{180}$ </p>\n",
    "\n",
    "It comes down to arround 111.195 km per degree of latitude.\n",
    "\n",
    "### Longitude\n",
    "For longitude - that is, the east-west location - it is a little bit trickier: since the distance between each longitude line decreases as you approach the poles, there is an additional factor. An easy way to understand it is that it is basically the same as for latitude, we are after all meassuring an arc length, but the radius of the circle is smaller. It is smaller, in fact, by a factor of $cos(\\phi)$. So, if $\\theta$ is is the longitude, then the distance between two given longitudes is\n",
    "\n",
    "<p style=\"text-align: center;\"> $d_{lng} = R\\Delta\\theta cos(\\phi) \\frac{\\pi}{180}$ </p>\n",
    "\n",
    "Which, again, can be simplified to 111.195$cos(\\phi)$ km per degree of longitude.\n",
    "\n",
    "The distance $d$ travelled, then, is approximately\n",
    "\n",
    "<p style=\"text-align: center;\"> $d\\approx \\sqrt{d_{lat}^2+d_{lng}^2}$ </p>\n",
    "\n",
    "and it is valid for short distances, where we can neglect the curvature of the earth. Distances within a city certainly meet this criteria.\n",
    "\n",
    "So lets define some functions to make this a bit cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b028744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_distance(df):\n",
    "    return (111.195 * (df[\"end_lat\"] - df[\"start_lat\"])) ** 2\n",
    "\n",
    "\n",
    "def lng_distance(df):\n",
    "    cos = np.cos(df[\"start_lat\"])\n",
    "    return (111.195 * cos * (df[\"end_lng\"] - df[\"start_lng\"])) ** 2\n",
    "\n",
    "\n",
    "def coord_distance(df):\n",
    "    distance_squared = lat_distance(df) + lng_distance(df)\n",
    "    return np.sqrt(distance_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050288bd",
   "metadata": {},
   "source": [
    "also, we only need the precission to go as far as meters, so the distance is rounded to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f237c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_clean[\"distance\"] = round(coord_distance(rides_clean), 2)\n",
    "rides_clean.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1e04a",
   "metadata": {},
   "source": [
    "Looks good. For a quick verification we get the range of distances traveled. It goes from 0 km, for trips that ended right where they started, to 36.03 km, which is a reasonable distance to ride in a few hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e682dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean[\"distance\"].agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc07dc",
   "metadata": {},
   "source": [
    "## Speed <a name=\"speed\"></a>\n",
    "\n",
    "It might be interesting to check if there are differences between the average speeds of members vs casual users. The average speed of each trip is given by the total distance traveled divided by the duration of the trip. Since the data has been purged from trips with a duration of less than a minute, there is no danger of dividing by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ba6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_clean.loc[rides_clean[\"minutes\"] > 0, \"speed\"] = round(\n",
    "    60 * rides_clean[\"distance\"] / rides_clean[\"minutes\"], 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_with_features = rides_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216d45f",
   "metadata": {},
   "source": [
    "# Outliers <a name=\"outliers\"></a>\n",
    "\n",
    "There is a 23 day trip in the data. This is enough evidence to sugest there may be some outliers lying arround. \n",
    "\n",
    "### duration, speed and distance <a name=\"dynamics\"></a>\n",
    "\n",
    "The next tables show some descrptive summary statistics for duration in minutes, distance traveled and speed.\n",
    "\n",
    "We see that the mean duration is considerably far appart from its median, sugesting there data for this field is highly skewed to the right. This is further verified by the really high standard deviation. Speed is better, but still has a somewhat large standard deviation. The plots below clarify a bit what is the situation with the speeds. For the distance, again, the difference between the mean and the median is considerable, and the standard deviation somewhat large, but nothing huge. \n",
    "\n",
    "The main concern here is then the duration, although all three fields need a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125804a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_with_features[[\"minutes\", \"speed\", \"distance\"]].agg([\"mean\", \"median\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f222cc",
   "metadata": {},
   "source": [
    "The next plots show, for the same three variables, its interquartile ranges, with the boxplots, as well as the overall distribution of their values, shown in the histograms. \n",
    "\n",
    "It is immediately noticeable that all 3 variables have numerous and significant outliers, the most dramatic being the duration. A close up for each plot shows the main body of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba718329",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2)\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "sns.boxplot(ax=ax[0, 0], data=rides_with_features[\"minutes\"])\n",
    "ax2 = plt.axes([0.17, 0.71, 0.1, 0.12])\n",
    "sns.boxplot(\n",
    "    ax=ax2, data=rides_with_features[rides_with_features[\"minutes\"] < 120][\"minutes\"]\n",
    ")\n",
    "ax2.set_ylim(0, 120)\n",
    "ax[0, 0].set_ylabel(\"Duration\", size=15)\n",
    "\n",
    "sns.histplot(ax=ax[0, 1], data=rides_with_features[\"minutes\"], bins=50)\n",
    "ax2 = plt.axes([0.69, 0.72, 0.15, 0.12])\n",
    "sns.histplot(\n",
    "    ax=ax2,\n",
    "    data=rides_with_features[rides_with_features[\"minutes\"] < 120][\"minutes\"],\n",
    "    bins=50,\n",
    ")\n",
    "ax2.set_xlim(0, 120)\n",
    "\n",
    "sns.boxplot(ax=ax[1, 0], data=rides_with_features[\"speed\"])\n",
    "ax2 = plt.axes([0.17, 0.46, 0.1, 0.12])\n",
    "sns.boxplot(\n",
    "    ax=ax2, data=rides_with_features[rides_with_features[\"speed\"] < 40][\"speed\"]\n",
    ")\n",
    "ax2.set_ylim(0, 40)\n",
    "ax[1, 0].set_ylabel(\"Speed\", size=15)\n",
    "\n",
    "sns.histplot(ax=ax[1, 1], data=rides_with_features[\"speed\"], bins=50)\n",
    "ax2 = plt.axes([0.69, 0.46, 0.15, 0.12])\n",
    "sns.histplot(\n",
    "    ax=ax2,\n",
    "    data=rides_with_features[rides_with_features[\"speed\"] < 40][\"speed\"],\n",
    "    bins=50,\n",
    ")\n",
    "ax2.set_xlim(0, 40)\n",
    "\n",
    "sns.boxplot(ax=ax[2, 0], data=rides_with_features[\"distance\"])\n",
    "ax2 = plt.axes([0.17, 0.18, 0.1, 0.12])\n",
    "sns.boxplot(\n",
    "    ax=ax2, data=rides_with_features[rides_with_features[\"distance\"] < 40][\"distance\"]\n",
    ")\n",
    "ax2.set_ylim(0, 10)\n",
    "ax[2, 0].set_ylabel(\"Distance\", size=15)\n",
    "\n",
    "sns.histplot(ax=ax[2, 1], data=rides_with_features[\"distance\"], bins=50)\n",
    "ax2 = plt.axes([0.68, 0.18, 0.15, 0.12])\n",
    "sns.histplot(\n",
    "    ax=ax2,\n",
    "    data=rides_with_features[rides_with_features[\"distance\"] < 40][\"distance\"],\n",
    "    bins=50,\n",
    ")\n",
    "ax2.set_xlim(0, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ae8bf",
   "metadata": {},
   "source": [
    "Since we are interested in the majority of users, we can trim the data to avoid outliers. The code below shows that by keeping the records that lie within 3 standard deviations of the mean for duration and speed and trips with a length of less than 10 km we are throwing out at most 1.638% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = len(rides_with_features[rides_with_features[\"minutes\"] > 120])\n",
    "speed = len(rides_with_features[rides_with_features[\"speed\"] > 24])\n",
    "dist = len(rides_with_features[rides_with_features[\"distance\"] > 10])\n",
    "\n",
    "100 * (mins + speed + dist) / len(rides_with_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410ee58",
   "metadata": {},
   "source": [
    "So lets proceed with filtering out those records (I didn't use exacly 3 standard deviations since part of the tails of the histograms were high enough to stil be statistically significant. For the duration variable it was the opposite, so I used less than 3 std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_filtered = rides_with_features[rides_with_features[\"minutes\"] <= 100]\n",
    "rides_filtered = rides_filtered[rides_filtered[\"speed\"] <= 30]\n",
    "rides_filtered = rides_filtered[rides_filtered[\"distance\"] <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8405439",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2)\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "sns.boxplot(ax=ax[0, 0], data=rides_filtered, x=\"minutes\")\n",
    "ax[0, 0].set_ylabel(\"Duration\", size=15)\n",
    "\n",
    "sns.histplot(ax=ax[0, 1], data=rides_filtered[\"minutes\"], bins=50)\n",
    "\n",
    "sns.boxplot(ax=ax[1, 0], data=rides_filtered, x=\"speed\")\n",
    "ax[1, 0].set_ylabel(\"Speed\", size=15)\n",
    "\n",
    "sns.histplot(ax=ax[1, 1], data=rides_filtered[\"speed\"], bins=50)\n",
    "\n",
    "sns.boxplot(ax=ax[2, 0], data=rides_filtered, x=\"distance\")\n",
    "ax[2, 0].set_ylabel(\"Distance\", size=15)\n",
    "\n",
    "sns.histplot(ax=ax[2, 1], data=rides_filtered[\"distance\"], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737143c0",
   "metadata": {},
   "source": [
    "### Start station coordinates <a name=\"start_coords\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd82557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_filtered[[\"start_lat\", \"start_lng\"]].agg([\"mean\", \"median\", \"std\", \"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "sns.boxplot(ax=ax[0, 0], data=rides_filtered[\"start_lat\"])\n",
    "sns.histplot(ax=ax[0, 1], data=rides_filtered, y=\"start_lat\", bins=50)\n",
    "ax[0, 1].set_ylim(41.5, 42.3)\n",
    "ax[0, 0].set_ylabel(\"Start Latitude\", size=14)\n",
    "\n",
    "sns.boxplot(ax=ax[1, 0], data=rides_filtered, x=\"start_lng\")\n",
    "sns.histplot(ax=ax[1, 1], data=rides_filtered[\"start_lng\"], bins=50)\n",
    "ax[1, 1].set_xlim(-87.9, -87.5)\n",
    "ax[1, 0].set_ylabel(\"Start Longitude\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377e2c4",
   "metadata": {},
   "source": [
    "### End station coordinates <a name=\"end_coords\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_filtered[[\"end_lat\", \"end_lng\"]].agg([\"mean\", \"median\", \"std\", \"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47869bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "sns.boxplot(ax=ax[0, 0], data=rides_filtered[\"end_lat\"])\n",
    "sns.histplot(ax=ax[0, 1], data=rides_filtered, y=\"end_lat\", bins=50)\n",
    "ax[0, 1].set_ylim(41.5, 42.3)\n",
    "ax[0, 0].set_ylabel(\"End Latitude\", size=14)\n",
    "\n",
    "sns.boxplot(ax=ax[1, 0], data=rides_filtered, x=\"end_lng\")\n",
    "sns.histplot(ax=ax[1, 1], data=rides_filtered[\"end_lng\"], bins=50)\n",
    "ax[1, 1].set_xlim(-87.9, -87.5)\n",
    "ax[1, 0].set_ylabel(\"End Longitude\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3515877",
   "metadata": {},
   "source": [
    "The range for the end station coordinates is larger than that of the start station's, and there are more evident outliers, especially for the longitude. To keep things simple we can restrict the range of coordinates to be that of the start stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7eef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = rides_filtered[\"start_lat\"].min()\n",
    "max_lat = rides_filtered[\"start_lat\"].max()\n",
    "min_lng = rides_filtered[\"start_lng\"].min()\n",
    "max_lng = rides_filtered[\"start_lng\"].max()\n",
    "\n",
    "rides_filtered = rides_filtered[\n",
    "    (rides_filtered[\"end_lat\"] >= min_lat) & (rides_filtered[\"end_lat\"] <= max_lat)\n",
    "]\n",
    "rides_filtered = rides_filtered[\n",
    "    (rides_filtered[\"end_lng\"] >= min_lng) & (rides_filtered[\"end_lng\"] <= max_lng)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38621ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_filtered[[\"end_lat\", \"end_lng\"]].agg([\"mean\", \"median\", \"std\", \"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89976673",
   "metadata": {},
   "source": [
    "# Verification <a name=\"verification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e667560",
   "metadata": {},
   "source": [
    "Now that we have a final cleaned version of the data it is a good time to verify that there are no missing values and that the number of unique values for each field and the ranges for the numeric variables make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_filtered.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_filtered.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcdc63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3213841",
   "metadata": {},
   "source": [
    "Now everything is filtered and a bit more consistent, although there are some instances of people biking at more than 200 km/h. This is definitely something to look at later. \n",
    "\n",
    "For now, to clear up things a bit we can delete all versions of the dataframe that wont be used anymore, and reinstate the original name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = rides_filtered.copy()\n",
    "del rides_filtered, rides_homogenized, rides_with_features, rides_clean, rides_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a17b1",
   "metadata": {},
   "source": [
    "And, finally, we need to save the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56493ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.to_csv(\"../data/cleaned/rides.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f9826",
   "metadata": {},
   "source": [
    "In total we lost %3.900 of the data during cleaning. Not too bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_data.shape[0] - rides.shape[0]) / all_data.shape[0] * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
